---
title: "Identify and Adjust for Non-response Bias"
author: "benjamin chu"
date: "3/8/2017"
output:
  html_document: default
  pdf_document: default
---
## Introduction
In biomath 204, we focused on data analysis techniques to draw useful information from given datasets. Another important aspect of data analysis is surveying. If we are unaware of bad surveying techniques and potential biases, we could work with a terrible dataset and get nothing (junk in junk out) at best. At worst, we could give wrong recommendations even though our analysis is completely correct. Among the pool of surveying techniques and biases, I focused on how to identify and adjust for $\textbf{non-response bias}$ in the context of a $\textbf{random sampling}$ survey, both of which are standard and compelling issues today.

* Random sampling - a subset of individuals (sample) chosen from a larget set (population).
* Non-response bias - Error due to the subset of the $\textbf{chosen sample}$ not responding to the survey. 

## Method and Data Description
From [Survey Documentation and Analysis (SDA) archive](http://sda.berkeley.edu/archive.htm), I obtained the [1991 Race and Politics Survey](http://sda.berkeley.edu/D3/Natlrace/Doc/nrac.htm) results. This is a telephone survey which collected a total of 2223 respondents with an impressive 65.3% response rate. This dataset is particularly suited for non-response bias analysis, because participants were sent additional survey questionnaires through mail after they completed the phone interview, in which the researchers received another 1198 "second round" response. Because the first round of telephone survey already collected certain information about the respondents, we can view this second round of sampling as a separate survey, and develop weights to adjust for non-response bias using demographic data collected from the first round. 

Interestingly, I did not find any published paper on this dataset, which is very strange in my opinion. 

## Identifying Non-response Bias through Callback
In the selected sample, people who did not participate in the survey (e.g. refusal, not at home...etc) may be randomly asked again to participate some time later. Those who agreed to take the survey only after several attempts are called late respondents. In survey methodology, this process is called $\textbf{callback}$ and it is an excellent way to identify non-response bias is to compare initial and late respondents. We must assume:

* These late respondents are similar to the non-respondents
* The difference between early and late respondents is captured in the metric we used to measure them. 

Among the 2223 respondents, we can determine how many people refused to participate in the survey at least 2 times:
```{r cache=FALSE, message=FALSE}
survey <- read.table(file="data2.txt", sep=",", header=T)
sum(survey$rcnt==0)
sum(survey$rcnt>=2)
```
Thus there were 1840 people who agreed to take the survey when they were first reached, and 62 people who eventually took the survey despite refusing to do so at least 2 times. Let us compare whether they responded differently to the question "Rules are to follow, not change".

```{r cache=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(sqldf)
test <- read.table(file="data2.txt", sep=",", header=T)
test$con2 <- factor(test$con2) # converts to a categorical variable
test$rcnt <- factor(test$rcnt)

test=sqldf("select CASEID,
  CASE WHEN rcnt==0 THEN '0 refusal (1840)'
       WHEN rcnt==1 THEN '1 refusal (321)'
       WHEN rcnt>=2 THEN '2+ refusal (62)'
  END rcnt,
  CASE WHEN con2==1 THEN '1) Strongly agree' 
       WHEN con2==3 THEN '2) Somewhat agree' 
       WHEN con2==5 THEN '3) Somewhat disagree' 
       WHEN con2==7 THEN '4) Strongly disagree' 
       WHEN con2==8 THEN '5) Not sure' 
       WHEN con2==9 THEN '6) Refuse to answer' 
  END con2 from test")

p = ggplot(data=test, aes(x="", stat="bin", fill=con2)) + geom_bar(position="fill")
p = p + ggtitle("Rules are to follow, not to change") + ylab("") + labs(fill='') + xlab("Number of refusals before answering survey")
p = p + facet_grid(facets=. ~ rcnt) # Side by side bar chart
p
```

As we can see from the bar graph above, the distribution of opinions among initial and late respondents is not exactly the same. In particular, the proportion of people who refused to answer this question is significantly higher in the late respondents than initial respondents. Is this statistically significant? Let us test out another example.

```{r cache=FALSE, message=FALSE, warning=FALSE}
test <- read.table(file="data2.txt", sep=",", header=T)
test$ef5 <- factor(test$ef5) # converts to a categorical variable
test$rcnt <- factor(test$rcnt)

test=sqldf("select CASEID,
  CASE WHEN rcnt==0 THEN '0 refusal (1840)'
       WHEN rcnt==1 THEN '1 refusal (321)'
       WHEN rcnt>=2 THEN '2+ refusal (62)'
  END rcnt,
  CASE WHEN ef5==1 THEN '1) Strongly agree' 
       WHEN ef5==3 THEN '2) Somewhat agree' 
       WHEN ef5==5 THEN '3) Somewhat disagree' 
       WHEN ef5==7 THEN '4) Strongly disagree' 
       WHEN ef5==8 THEN '5) Not sure' 
       WHEN ef5==9 THEN '6) Refuse to answer' 
  END ef5 from test")

p = ggplot(data=test, aes(x="", stat="bin", fill=ef5)) + geom_bar(position="fill")
p = p + ggtitle("Should we narrow the gap between rich and poor?") + ylab("") + labs(fill='') + xlab("Number of refusals before answering survey")
p = p + facet_grid(facets=. ~ rcnt) # Side by side bar chart
p
```

## Discussion
hihihihihi

## References

* http://sda.berkeley.edu/D3/Natlrace/Doc/nrac.htm
* http://sda.berkeley.edu/archive.htm
* http://www.trchome.com/research-knowledge/white-paper-library/227-situational-use-of-data-weighting-complete
* http://www.trchome.com/65-market-research-knowledge/white-paper-library/215-non-response-bias-in-survey-sampling-complete